{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgupta/anaconda3/envs/pytenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import pathlib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_path = './data/graph_data/2_hr_time_window_dynamic_graph/'\n",
    "pathlib.Path(op_path).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_ids = [3,4,5,6,7,8,9,14,17,18,19,22,23,28,29,34,41,42,50,52,55,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "for route_id in route_ids:\n",
    "    inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/route_specific_w_census_dist_delay_weather_traffic/{route_id}'\n",
    "    filename = f'stoplevel_route_{route_id}_w_census_dist_delay_weather_traffic.parquet'\n",
    "    df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "    \n",
    "    all_df.append(df)\n",
    "\n",
    "all_df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         2\n",
       "2         2\n",
       "3         2\n",
       "4         2\n",
       "         ..\n",
       "846789    3\n",
       "846790    3\n",
       "846791    3\n",
       "846792    3\n",
       "846793    3\n",
       "Name: dayofweek, Length: 5975933, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: on all data. \n",
    "# Converting 'precipitation_intensity','temperature','humidity' scaling them between 0 and 1 for the model to understand these values \n",
    "num_columns = ['precipitation_intensity','temperature','humidity','actual_hdwy', 'delay','displacement','median_income_last12months','average_speed']\n",
    "    \n",
    "\n",
    "ss = MinMaxScaler()\n",
    "ss.fit(all_df[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_df\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_2_hour_window(window):\n",
    "    window = (window // 4)\n",
    "\n",
    "    if(window == 12):\n",
    "        return 0\n",
    "    else:\n",
    "        return window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9463/9463 [00:36<00:00, 260.93it/s]\n",
      "100%|██████████| 5475/5475 [00:01<00:00, 4824.56it/s]\n",
      "100%|██████████| 2721/2721 [00:10<00:00, 268.76it/s]\n",
      "100%|██████████| 2827/2827 [00:00<00:00, 4358.04it/s]\n",
      "100%|██████████| 2800/2800 [00:10<00:00, 259.66it/s]\n",
      "100%|██████████| 2517/2517 [00:00<00:00, 9090.90it/s]\n",
      "100%|██████████| 2976/2976 [00:12<00:00, 247.78it/s]\n",
      "100%|██████████| 2958/2958 [00:00<00:00, 3856.60it/s]\n",
      "100%|██████████| 10504/10504 [00:38<00:00, 275.68it/s]\n",
      "100%|██████████| 6111/6111 [00:01<00:00, 4781.31it/s]\n",
      "100%|██████████| 7494/7494 [00:27<00:00, 271.50it/s]\n",
      "100%|██████████| 5572/5572 [00:01<00:00, 3769.42it/s]\n",
      "100%|██████████| 1887/1887 [00:06<00:00, 270.51it/s]\n",
      "100%|██████████| 1462/1462 [00:00<00:00, 9117.27it/s]\n",
      "100%|██████████| 2083/2083 [00:07<00:00, 264.12it/s]\n",
      "100%|██████████| 2101/2101 [00:00<00:00, 6283.23it/s]\n",
      "100%|██████████| 1487/1487 [00:05<00:00, 272.95it/s]\n",
      "100%|██████████| 1417/1417 [00:00<00:00, 5602.41it/s]\n",
      "100%|██████████| 3522/3522 [00:13<00:00, 260.86it/s]\n",
      "100%|██████████| 3430/3430 [00:00<00:00, 4137.12it/s]\n",
      "100%|██████████| 4352/4352 [00:16<00:00, 263.49it/s]\n",
      "100%|██████████| 4027/4027 [00:00<00:00, 4224.89it/s]\n",
      "100%|██████████| 7762/7762 [00:28<00:00, 269.91it/s]\n",
      "100%|██████████| 5347/5347 [00:01<00:00, 4795.20it/s]\n",
      "100%|██████████| 18501/18501 [01:10<00:00, 263.79it/s]\n",
      "100%|██████████| 9948/9948 [00:01<00:00, 7891.36it/s] \n",
      "100%|██████████| 719/719 [00:02<00:00, 288.20it/s]\n",
      "100%|██████████| 860/860 [00:00<00:00, 9740.50it/s]\n",
      "100%|██████████| 653/653 [00:02<00:00, 285.11it/s]\n",
      "100%|██████████| 662/662 [00:00<00:00, 4250.85it/s]\n",
      "100%|██████████| 2897/2897 [00:10<00:00, 267.10it/s]\n",
      "100%|██████████| 2737/2737 [00:00<00:00, 3580.18it/s]\n",
      "100%|██████████| 379/379 [00:01<00:00, 281.88it/s]\n",
      "100%|██████████| 468/468 [00:00<00:00, 10856.76it/s]\n",
      "100%|██████████| 1793/1793 [00:06<00:00, 272.63it/s]\n",
      "100%|██████████| 1747/1747 [00:00<00:00, 4446.96it/s]\n",
      "100%|██████████| 12794/12794 [00:47<00:00, 271.28it/s]\n",
      "100%|██████████| 7568/7568 [00:01<00:00, 4682.09it/s]\n",
      "100%|██████████| 24219/24219 [01:34<00:00, 256.32it/s]\n",
      "100%|██████████| 10020/10020 [00:01<00:00, 7513.38it/s]\n",
      "100%|██████████| 28541/28541 [01:49<00:00, 259.76it/s]\n",
      "100%|██████████| 11141/11141 [00:02<00:00, 4400.16it/s]\n",
      "100%|██████████| 24891/24891 [01:35<00:00, 261.36it/s]\n",
      "100%|██████████| 10983/10983 [00:02<00:00, 4149.72it/s]\n",
      "100%|██████████| 22/22 [14:21<00:00, 39.17s/it]\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "node_feature_matrix_graphs = []\n",
    "y_class_graphs = []\n",
    "\n",
    "for route_id in tqdm(route_ids):\n",
    "    inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/route_specific_w_census_dist_delay_weather_traffic/{route_id}'\n",
    "    filename = f'stoplevel_route_{route_id}_w_census_dist_delay_weather_traffic.parquet'\n",
    "    df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "\n",
    "    # Creating 2 hour time_window\n",
    "    df['time_window_2_hr'] = df['time_window'].apply(map_to_2_hour_window)\n",
    "\n",
    "    df = df.drop_duplicates(subset=['transit_date', 'trip_id', 'route_direction_name', 'block_abbr', 'pattern_num','stop_sequence'])\n",
    "    df['delay'] = df['delay'].fillna(0)\n",
    "    df['actual_hdwy'] = df['actual_hdwy'].fillna(0)\n",
    "\n",
    "    # Converting route_direction_name, is_weekend and is_holiday to OHE - they have only 2 values hence just mapping them to 0 and 1.\n",
    "    # Create a direction mapping dictionary\n",
    "    direction_mapping = {'TO DOWNTOWN': 1, 'FROM DOWNTOWN': 0}\n",
    "    # Apply the mapping to the 'direction' column\n",
    "    df['route_direction_name'] = df['route_direction_name'].map(direction_mapping)\n",
    "\n",
    "    # Create a true false mapping dictionary\n",
    "    true_false_mapping = {True: 1, False: 0}\n",
    "    # Apply the mapping to the holiday and weekend columns\n",
    "    df['is_holiday'] = df['is_holiday'].map(true_false_mapping)\n",
    "    df['is_weekend'] = df['is_weekend'].map(true_false_mapping)\n",
    "\n",
    "    # Normalising some of the columns as they are percentage values: 'white_pct', 'black_pct', 'hispanic_pct', 'public_transit_pct'\n",
    "    df['white_pct'] = df['white_pct']/100\n",
    "    df['black_pct'] = df['black_pct']/100\n",
    "    df['hispanic_pct'] = df['hispanic_pct']/100\n",
    "    df['public_transit_pct'] = df['public_transit_pct']/100\n",
    "\n",
    "    ## No Normalising required for 'pct_public_transit_for_work', 'extreme_congestion' as it is between 0 and 1 already\n",
    "    df[num_columns] = ss.transform(df[num_columns])\n",
    "\n",
    "    # Getting the source information\n",
    "    df['source'] = df['stop_id']\n",
    "\n",
    "    df.sort_values(by=['transit_date', 'trip_id', 'route_direction_name', 'block_abbr', 'pattern_num','stop_sequence'],inplace=True,ignore_index=True)\n",
    "\n",
    "    # Creating the target column for all the trips in our dataset\n",
    "    req_df = []\n",
    "    for (transit_date, trip_id, route_direction_name, block_abbr, pattern_num), tdf in tqdm(df.groupby(['transit_date', 'trip_id','route_direction_name', 'block_abbr', 'pattern_num'])):   \n",
    "        tdf.sort_values(by=['transit_date', 'trip_id', 'route_direction_name', 'block_abbr', 'pattern_num','stop_sequence'],inplace=True,ignore_index=True)\n",
    "        if tdf.vehicle_capacity.isna().any():\n",
    "            continue\n",
    "\n",
    "\n",
    "        tdf['target'] = tdf['source'].shift(-1).astype(int, errors = 'ignore')\n",
    "        # tdf = tdf.dropna(subset=['target'])\n",
    "        tdf['target'] = tdf['target'].fillna('DELETE_NODE')\n",
    "\n",
    "        temp = []\n",
    "        for (time_window), time_window_df in tdf.groupby(['time_window_2_hr']):\n",
    "            if time_window_df['target'].iloc[-1] != 'DELETE_NODE':\n",
    "                time_window_df['target'].iloc[-1] = 'DELETE_NODE'\n",
    "\n",
    "            temp.append(time_window_df)\n",
    "\n",
    "        req_df.append(pd.concat(temp))\n",
    "\n",
    "    df = pd.concat(req_df)\n",
    "    del req_df\n",
    "\n",
    "    df.sort_values(by=['transit_date', 'route_direction_name' ,'time_window_2_hr'],ignore_index=True,inplace=True)\n",
    "\n",
    "    # changing the target of the last row for each 'transit_date', 'route_direction_name' ,'time_window' because this row will be present in the graph for the next day. \n",
    "    fixed_df = []\n",
    "    for (transit_date, route_direction_name, time_window), tdf in tqdm(df.groupby(['transit_date', 'route_direction_name' ,'time_window_2_hr'])):   \n",
    "        if tdf['target'].iloc[-1] != 'DELETE_NODE':\n",
    "            tdf['target'].iloc[-1] = 'DELETE_NODE'\n",
    "\n",
    "        fixed_df.append(tdf)\n",
    "\n",
    "    df = pd.concat(fixed_df)\n",
    "    op_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/graph_specific_data/{route_id}'\n",
    "    pathlib.Path(op_fp).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    filename = f'graph_ready_data_{route_id}_2_hr_time_window.parquet'\n",
    "    df.to_parquet(f'{op_fp}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24891/24891 [00:08<00:00, 2904.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fixing route 23 data - \n",
    "inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/graph_specific_data/{23}'\n",
    "filename = f'graph_ready_data_{23}.parquet'\n",
    "fix_23_df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "\n",
    "route_23_df = []\n",
    "for (transit_date, trip_id, route_direction_name, block_abbr, pattern_num), tdf in tqdm(df.groupby(['transit_date', 'trip_id','route_direction_name', 'block_abbr', 'pattern_num'])):\n",
    "    if (tdf.source == tdf.target).any():\n",
    "        continue\n",
    "\n",
    "    route_23_df.append(tdf)\n",
    "\n",
    "route_23_df = pd.concat(route_23_df)\n",
    "\n",
    "op_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/graph_specific_data/{23}'\n",
    "filename = f'graph_ready_data_{23}_2_hr_time_window.parquet'\n",
    "route_23_df.to_parquet(f'{op_fp}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_data = []\n",
    "for route_id in route_ids:\n",
    "    inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/graph_specific_data/{route_id}'\n",
    "    filename = f'graph_ready_data_{route_id}_2_hr_time_window.parquet'\n",
    "\n",
    "    df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "\n",
    "    apc_data.append(df)\n",
    "\n",
    "apc_data = pd.concat(apc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_data.sort_values(by=['transit_date','route_direction_name','departure_time'],inplace=True,ignore_index=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ,'time_window_1_hr'\n",
    "categorical_columns = ['dayofweek','month','year','time_window_2_hr']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(apc_data[col].unique())\n",
    "\n",
    "    apc_data[f'{col}_cat'] = le.transform(apc_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Creating single graph for each time window - Dynamic Graph \n",
    "\n",
    "<!-- 2023-02-21\t32.0\t -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code plotting stuff for a single transit date - can be ignored for now \n",
    "\n",
    "# Code for getting the transit date with highest routes and time windows\n",
    "temp = apc_data.groupby(['transit_date'])['time_window_2_hr'].nunique()\n",
    "temp = temp.reset_index()\n",
    "temp.sort_values('time_window_2_hr',ascending=False)\n",
    "\n",
    "temp2 = apc_data.groupby(['transit_date'])['route_id'].nunique()\n",
    "temp2 = temp2.reset_index()\n",
    "\n",
    "temp3 = temp2.merge(temp,on='transit_date')\n",
    "temp3.sort_values(['route_id','time_window_2_hr'],ascending=False)\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_and_save_graph_with_mapbox(G,pos_df,transit_date,time_window):\n",
    "    pos = pos_df.to_dict(orient='index')\n",
    "    pos = {key: tuple(value.values()) for key, value in pos.items()}\n",
    "\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        # edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        # edge_y.append(None)\n",
    "\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_text.append(node)\n",
    "\n",
    "    # Create a DataFrame or provide edge_x, edge_y, node_x, node_y\n",
    "    # with the appropriate data for your plot.\n",
    "\n",
    "    # Create the edge DataFrame (You can replace this with your actual data)\n",
    "    edge_df = pd.DataFrame({'x': edge_x, 'y': edge_y})\n",
    "\n",
    "    # Create the node DataFrame (You can replace this with your actual data)\n",
    "    node_df = pd.DataFrame({'x': node_x, 'y': node_y, 'text': node_text})\n",
    "\n",
    "    # Create the map\n",
    "    fig = px.scatter_mapbox(node_df, lat=\"y\", lon=\"x\", hover_data=\"text\",\n",
    "                            color_discrete_sequence=[\"blue\"], zoom=10,\n",
    "                            center={\"lat\": 36.1627, \"lon\": -86.7816}, title=\"Network graph with Mapbox in Python\")\n",
    "\n",
    "    # Add the edges as lines (You can replace this with your actual data)\n",
    "    for i in range(0, len(edge_df), 2):\n",
    "        start_node = edge_df.iloc[i]\n",
    "        end_node = edge_df.iloc[i + 1]\n",
    "        line = pd.DataFrame({'x': [start_node['x'], end_node['x']], 'y': [start_node['y'], end_node['y']]})\n",
    "        fig.add_trace(px.line_mapbox(line, lat=\"y\", lon=\"x\").data[0])\n",
    "\n",
    "    # Customize the map layout\n",
    "    fig.update_layout(mapbox_style=\"light\",\n",
    "                    mapbox_accesstoken=\"pk.eyJ1IjoiZ3VwdGFzYW16IiwiYSI6ImNsZ3d6Zzh0eTAwbjMzcW8wcnJybmp6cmcifQ.4ZGZIjNSFzk6aYjYUT3P1Q\",  # Replace with your Mapbox access token\n",
    "                    mapbox_center={\"lat\": 36.1627, \"lon\": -86.7816},\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin={\"b\": 20, \"l\": 5, \"r\": 5, \"t\": 40},\n",
    "                    annotations=[\n",
    "                        dict(text=\"Python code: <a href='https://plotly.com/ipython-notebooks/network-graphs/'>https://plotly.com/ipython-notebooks/network-graphs/</a>\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002)\n",
    "                    ],\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "\n",
    "    op_fp = './plots/dynamic_graph_2_hr_time_window/graph_strucutre/with_mapbox'\n",
    "    pathlib.Path(op_fp).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "\n",
    "    # Save or display the figure\n",
    "    fig.write_html(f'{op_fp}/graph_with_mapbox_{transit_date}_{time_window}.html')\n",
    "\n",
    "def plot_and_save_graph_without_mapbox(G,pos_df,transit_date,time_window):\n",
    "    # pos = nx.spiral_layout(G)\n",
    "    pos = pos_df.to_dict(orient='index')\n",
    "    pos = {key: tuple(value.values()) for key, value in pos.items()}\n",
    "\n",
    "    fig = plt.figure(1, figsize=(200, 80), dpi=60)\n",
    "    nx.draw_networkx(G,font_size=100,node_size=30000, arrowsize=300, width=10,pos=pos)\n",
    "\n",
    "    op_fp = './plots/dynamic_graph_2_hr_time_window/graph_strucutre/without_mapbox'\n",
    "    pathlib.Path(op_fp).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "    plt.savefig(f'{op_fp}/graph_{transit_date}_{time_window}.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "t = apc_data[(apc_data.transit_date == '2021-10-26')]\n",
    "\n",
    "graphs = []\n",
    "count = 0\n",
    "time_window_count = 0\n",
    "# Creating the road network graph for a single days data for now. Here each node represents the stop and edge represents the stop that the bus travels to.\n",
    "# for (route_direction_name), tdf in tqdm(df.groupby(['route_direction_name' ])):   \n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(t.groupby(['transit_date', 'route_direction_name' , 'time_window_2_hr'])):   \n",
    "    # display(tdf)\n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # print('RDN: ',route_direction_name)\n",
    "    G=nx.from_pandas_edgelist(tdf, 'source', 'target', ['displacement','median_income_last12months',\n",
    "                                                    'white_pct', 'black_pct', 'hispanic_pct', 'public_transit_pct',\n",
    "                                                    'pct_public_transit_for_work'],create_using=nx.DiGraph())\n",
    "    \n",
    "    try:\n",
    "        G.remove_node('DELETE_NODE')\n",
    "    except:\n",
    "        print('iteration:',count)\n",
    "\n",
    "    graphs.append(G)\n",
    "    \n",
    "    pos_df = tdf.groupby(['stop_id']).agg({\n",
    "                                            'map_longitude':'first','map_latitude':'first'\n",
    "                                            })\n",
    "    \n",
    "    plot_and_save_graph_with_mapbox(G,pos_df,transit_date,time_window)\n",
    "\n",
    "    plot_and_save_graph_without_mapbox(G,pos_df,transit_date,time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13873/13873 [00:22<00:00, 604.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graphs = []\n",
    "count = 0\n",
    "time_window_count = 0\n",
    "# Creating the road network graph for a single days data for now. Here each node represents the stop and edge represents the stop that the bus travels to.\n",
    "# for (route_direction_name), tdf in tqdm(df.groupby(['route_direction_name' ])):   \n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(apc_data.groupby(['transit_date', 'route_direction_name' , 'time_window_2_hr'])):   \n",
    "    # display(tdf)\n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # print('RDN: ',route_direction_name)\n",
    "    G=nx.from_pandas_edgelist(tdf, 'source', 'target', ['displacement','median_income_last12months',\n",
    "                                                    'white_pct', 'black_pct', 'hispanic_pct', 'public_transit_pct',\n",
    "                                                    'pct_public_transit_for_work'],create_using=nx.DiGraph())\n",
    "    \n",
    "    try:\n",
    "        G.remove_node('DELETE_NODE')\n",
    "    except:\n",
    "        print('iteration:',count)\n",
    "\n",
    "    graphs.append(G)\n",
    "    \n",
    "    pos_df = tdf.groupby(['stop_id']).agg({\n",
    "                                            'map_longitude':'first','map_latitude':'first'\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the feature node matrix for the first graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_load(load, capacity):\n",
    "    percentages = [0., .33, .66, 1.0]\n",
    "    # percentages = [0., .10, .25, 1.0]\n",
    "\n",
    "    transit_cap = [round(p * capacity) for p in percentages]\n",
    "    labels = [0, 1, 2]\n",
    "    \n",
    "    bin_label = pd.cut(x=[load], bins=transit_cap, labels=labels, include_lowest=True)[0]\n",
    "    \n",
    "    if pd.isnull(bin_label):\n",
    "        return 3\n",
    "\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13873/13873 [19:34<00:00, 11.81it/s]\n"
     ]
    }
   ],
   "source": [
    "features = ['route_direction_name','pattern_num','is_weekend','month','year','is_holiday','vehicle_capacity','precipitation_intensity','temperature','humidity','time_window_2_hr'] \n",
    "features = ['route_direction_name','is_weekend','is_holiday','precipitation_intensity','temperature','humidity','time_window_2_hr','delay','average_speed','extreme_congestion'] \n",
    "features = ['precipitation_intensity','temperature','humidity','delay','average_speed','extreme_congestion',\n",
    "            'dayofweek','month','year','time_window_2_hr'] \n",
    "\n",
    "node_feature_matrix_graphs = []\n",
    "y_class_graphs = []\n",
    "# time_window_index = []\n",
    "\n",
    "iteration = 0\n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(apc_data.groupby(['transit_date','route_direction_name' ,'time_window_2_hr'])):   \n",
    "    # Create node feature matrix and the y_class matrix (Creating this as well as a dictionary for now)\n",
    "    # Designed as a dictionary. Traverse each row sequentially and get the aggregate (mean) of all the values \n",
    "    node_feature_matrix = {}\n",
    "    y_class = {}\n",
    "\n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # Setting the keys to maintain the sequential order with respect to nodes in graph\n",
    "    for key in list(graphs[iteration].nodes):\n",
    "        node_feature_matrix[key] = None\n",
    "        y_class[key] = None\n",
    "\n",
    "    for (stop_id), stop_df in tdf.groupby(['stop_id']):\n",
    "        stop_id = stop_df['stop_id'].iloc[0]\n",
    "        node_feature_matrix[stop_id] = {}\n",
    "        node_feature_matrix[stop_id]['dayofweek'] = stop_df.dayofweek_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['month'] = stop_df.month_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['year'] = stop_df.year_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['time_window_2_hr'] = stop_df.time_window_2_hr_cat.iloc[0] #Check whether this need to be one hot encoded \n",
    "\n",
    "        # Aggregate (mean) the Weather data - temp, precipitation, humidity. \n",
    "        node_feature_matrix[stop_id]['precipitation_intensity'] = stop_df.precipitation_intensity.mean()\n",
    "        node_feature_matrix[stop_id]['temperature'] = stop_df.temperature.mean()\n",
    "        node_feature_matrix[stop_id]['humidity'] = stop_df.humidity.mean()\n",
    "\n",
    "        # node_feature_matrix[stop_id]['actual_hdwy'] = stop_df.actual_hdwy.mean()\n",
    "        node_feature_matrix[stop_id]['delay'] = stop_df.delay.mean()\n",
    "\n",
    "        # Aggregate Traffic data after merging.\n",
    "        node_feature_matrix[stop_id]['average_speed'] = stop_df.average_speed.mean()\n",
    "        node_feature_matrix[stop_id]['extreme_congestion'] = stop_df.extreme_congestion.mean()\n",
    "\n",
    "        # Getting the binned load using mean load and mean vehicle capacity\n",
    "        mean_load = stop_df.load.max()\n",
    "        mean_vehicle_capacity = stop_df.vehicle_capacity.max()\n",
    "        binned_load = bin_load(mean_load,mean_vehicle_capacity)\n",
    "\n",
    "        # Getting the mean of the load - will bin it later using vehicle capacity\n",
    "        y_class[stop_id] = binned_load\n",
    "\n",
    "    node_feature_matrix_graphs.append(node_feature_matrix)\n",
    "    y_class_graphs.append(y_class)\n",
    "    iteration = iteration+1\n",
    "\n",
    "    # time_window_index.append(time_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/graph_data/2_hr_time_window_dynamic_graph/node_feature_matrix_single_graph.pkl\", 'wb') as fp:\n",
    "    pickle.dump(node_feature_matrix_graphs, fp)\n",
    "\n",
    "with open(\"./data/graph_data/2_hr_time_window_dynamic_graph/y_class_single_graph.pkl\", 'wb') as fp:\n",
    "    pickle.dump(y_class_graphs, fp)\n",
    "\n",
    "with open(\"./data/graph_data/2_hr_time_window_dynamic_graph/graph_single_graph.pkl\", 'wb') as fp:\n",
    "    pickle.dump(y_class_graphs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_class to list from dictionary\n",
    "y_class = list(y_class.values())\n",
    "\n",
    "# Converting node_feature_matrix to 2D matrix from dictionary of dictionaries\n",
    "for key in node_feature_matrix:\n",
    "    node_feature_matrix[key] = list(node_feature_matrix[key].values())\n",
    "node_feature_matrix = list(node_feature_matrix.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix shape: (0,)\n",
      "Number of nodes: 130\n",
      "Number of Features:  10\n"
     ]
    }
   ],
   "source": [
    "# Below value should equal (number of nodes, number of features)\n",
    "print(\"Feature Matrix shape:\",np.array(node_feature_matrix).shape)\n",
    "print(\"Number of nodes:\",len(G.nodes))\n",
    "print('Number of Features: ',len(features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1103/7162 [00:00<00:00, 11024.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREVANSM\n",
      "MAIFREEF\n",
      "FREVANSM\n",
      "JAM8AWN\n",
      "LAFFAIEN\n",
      "HER12AWN\n",
      "BAT29AWN\n",
      "JAM8AWN\n",
      "HAR8AWF\n",
      "HERDRISF\n",
      "MXOELMHI\n",
      "JAM8AWN\n",
      "HER12AWN\n",
      "BAT29AWN\n",
      "HER14AWN\n",
      "HER12AWN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 4269/7162 [00:00<00:00, 9946.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6AVCHUSN\n",
      "HARTAMEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 6213/7162 [00:00<00:00, 8559.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREVANSM\n",
      "JAM2AEF\n",
      "4AVCOMSN\n",
      "5SRUSSN\n",
      "WHIPOSNN\n",
      "HPKL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7162/7162 [00:00<00:00, 8976.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAHAYEN\n",
      "HPKL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in tqdm(range(len(node_feature_matrix_graphs))):\n",
    "    # Converting y_class to list from dictionary\n",
    "    y_class_graphs[iteration] = list(y_class_graphs[iteration].values())\n",
    "\n",
    "    # Converting node_feature_matrix to 2D matrix from dictionary of dictionaries\n",
    "    for key in node_feature_matrix_graphs[iteration]:\n",
    "        try:\n",
    "            node_feature_matrix_graphs[iteration][key] = list(node_feature_matrix_graphs[iteration][key].values())\n",
    "        except:\n",
    "            print(key)\n",
    "    node_feature_matrix_graphs[iteration] = list(node_feature_matrix_graphs[iteration].values())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs:  7162\n",
      "Number of node_feature_matrix_graphs:  7162\n",
      "Number of y_class_graphs:  7162\n"
     ]
    }
   ],
   "source": [
    "# Print some stats\n",
    "print(\"Number of graphs: \",len(graphs))\n",
    "print(\"Number of node_feature_matrix_graphs: \",len(node_feature_matrix_graphs))\n",
    "print(\"Number of y_class_graphs: \",len(y_class_graphs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the networkx graph to pygeometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing values for None rows \n",
    "\n",
    "for i in range(len(node_feature_matrix_graphs)):\n",
    "    for j in range(len(node_feature_matrix_graphs[i])):\n",
    "        # print(node_feature_matrix_graphs[i][j])\n",
    "        if(node_feature_matrix_graphs[i][j] is None):\n",
    "            node_feature_matrix_graphs[i][j] = np.zeros(len(features))\n",
    "        else:\n",
    "            node_feature_matrix_graphs[i][j] = np.array(node_feature_matrix_graphs[i][j])\n",
    "\n",
    "\n",
    "for i in range(len(y_class_graphs)):\n",
    "    # print(y_class)\n",
    "    for j in range(len(y_class_graphs[i])):\n",
    "        if(y_class_graphs[i][j] is None):\n",
    "            y_class_graphs[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 723418, '1': 456089, '2': 69470, '3': 22383, '4': 0, 'None': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict = {\n",
    "    '0':0,\n",
    "    '1':0,\n",
    "    '2':0,\n",
    "    '3':0,\n",
    "    '4':0,\n",
    "    'None':0\n",
    "}\n",
    "\n",
    "for i in range(len(y_class_graphs)):\n",
    "    # print(y_class)\n",
    "    for j in range(len(y_class_graphs[i])):\n",
    "        y_dict[str(y_class_graphs[i][j])] += 1\n",
    "\n",
    "y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7162/7162 [00:10<00:00, 683.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These graphs were removed because there was only a single node in these graphs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the resulting dataframe with all the pygeometric graph data\n",
    "dataset_all_routes = []\n",
    "\n",
    "count = 0\n",
    "for iteration in tqdm(range(len(graphs))):\n",
    "\n",
    "    try:\n",
    "        # Creating the py geometric graph from networkx graph\n",
    "        pyg_graph = from_networkx(graphs[iteration],group_edge_attrs=['displacement','median_income_last12months',\n",
    "                                                                        'white_pct', 'black_pct', 'hispanic_pct', \n",
    "                                                                        'public_transit_pct', 'pct_public_transit_for_work'])\n",
    "        # Setting the node feature matrix for the py geometric graph \n",
    "        pyg_graph.x = torch.tensor(node_feature_matrix_graphs[iteration]).float()\n",
    "        # Setting the y_class for the py geometric graph \n",
    "        pyg_graph.y = torch.tensor(y_class_graphs[iteration]).long()\n",
    "\n",
    "        dataset_all_routes.append(pyg_graph)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        count = count + 1\n",
    "        continue\n",
    "        # print(\"Some issue with graph: \",iteration)\n",
    "\n",
    "print(\"These graphs were removed because there was only a single node in these graphs:\",count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Number of graphs: 7147\n",
      "Number of features: 10\n",
      "Number of classes: 4\n",
      "\n",
      "Data(edge_index=[2, 49], edge_attr=[49, 7], num_nodes=50, x=[50, 10], y=[50])\n",
      "=============================================================\n",
      "Number of nodes: 50\n",
      "Number of edges: 49\n",
      "Average node degree: 0.98\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = dataset_all_routes[0]  # Get the first graph object.\n",
    "# Hard coding for now the code after ends here can be used to derieve this\n",
    "num_classes = 4\n",
    "num_features = len(features)\n",
    "\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset_all_routes)}')\n",
    "print(f'Number of features: {num_features}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final data\n",
    "op_path = './data/graph_data/2_hr_time_window_dynamic_graph'\n",
    "op_filename = 'dataset_all_route_dynamic_graph.pt'\n",
    "\n",
    "torch.save(dataset_all_routes, f'{op_path}/{op_filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
