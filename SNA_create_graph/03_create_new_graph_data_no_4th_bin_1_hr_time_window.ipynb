{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgupta/anaconda3/envs/pytenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import pathlib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_ids = [3,4,5,6,7,8,9,14,17,18,19,22,23,28,29,34,41,42,50,52,55,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "for route_id in route_ids:\n",
    "    inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/route_specific_w_census_dist_delay_weather_traffic/{route_id}'\n",
    "    filename = f'stoplevel_route_{route_id}_w_census_dist_delay_weather_traffic.parquet'\n",
    "    df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "    \n",
    "    all_df.append(df)\n",
    "\n",
    "all_df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_path = './data/graph_data/1_hr_time_window_dynamic_graph/'\n",
    "pathlib.Path(op_path).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/graph_data/1_hr_time_window_dynamic_graph/num_features.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: on all data. \n",
    "# Converting 'precipitation_intensity','temperature','humidity' scaling them between 0 and 1 for the model to understand these values \n",
    "num_columns = ['precipitation_intensity','temperature','humidity','actual_hdwy', 'delay','displacement','median_income_last12months','average_speed']\n",
    "    \n",
    "\n",
    "ss = MinMaxScaler()\n",
    "ss.fit(all_df[num_columns])\n",
    "\n",
    "import joblib\n",
    "joblib.dump(ss,'./data/graph_data/1_hr_time_window_dynamic_graph/num_features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_1_hour_window(window):\n",
    "    window = (window // 2)\n",
    "\n",
    "    if(window == 12):\n",
    "        return 0\n",
    "    else:\n",
    "        return window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_df\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_data = []\n",
    "for route_id in route_ids:\n",
    "    inp_fp = f'/home/sgupta/WORK/DATASETS/WeGo_Bus_data/STOPLEVEL_PROCESSED/graph_specific_data/{route_id}'\n",
    "    filename = f'graph_ready_data_{route_id}_1_hr_time_window.parquet'\n",
    "\n",
    "    df = pd.read_parquet(f'{inp_fp}/{filename}')\n",
    "\n",
    "    apc_data.append(df)\n",
    "\n",
    "apc_data = pd.concat(apc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_data.sort_values(by=['transit_date','route_direction_name','departure_time'],inplace=True,ignore_index=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ,'time_window_1_hr'\n",
    "categorical_columns = ['dayofweek','month','year','time_window_1_hr']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(apc_data[col].unique())\n",
    "\n",
    "    apc_data[f'{col}_cat'] = le.transform(apc_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Creating single graph for each time window - Dynamic Graph \n",
    "\n",
    "<!-- 2023-02-21\t32.0\t -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code plotting stuff for a single transit date - can be ignored for now \n",
    "\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_and_save_graph_with_mapbox(G,pos_df,transit_date,time_window):\n",
    "    pos = pos_df.to_dict(orient='index')\n",
    "    pos = {key: tuple(value.values()) for key, value in pos.items()}\n",
    "\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        # edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        # edge_y.append(None)\n",
    "\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_text.append(node)\n",
    "\n",
    "    # Create a DataFrame or provide edge_x, edge_y, node_x, node_y\n",
    "    # with the appropriate data for your plot.\n",
    "\n",
    "    # Create the edge DataFrame (You can replace this with your actual data)\n",
    "    edge_df = pd.DataFrame({'x': edge_x, 'y': edge_y})\n",
    "\n",
    "    # Create the node DataFrame (You can replace this with your actual data)\n",
    "    node_df = pd.DataFrame({'x': node_x, 'y': node_y, 'text': node_text})\n",
    "\n",
    "    # Create the map\n",
    "    fig = px.scatter_mapbox(node_df, lat=\"y\", lon=\"x\", hover_data=\"text\",\n",
    "                            color_discrete_sequence=[\"blue\"], zoom=10,\n",
    "                            center={\"lat\": 36.1627, \"lon\": -86.7816}, title=\"Network graph with Mapbox in Python\")\n",
    "\n",
    "    # Add the edges as lines (You can replace this with your actual data)\n",
    "    for i in range(0, len(edge_df), 2):\n",
    "        start_node = edge_df.iloc[i]\n",
    "        end_node = edge_df.iloc[i + 1]\n",
    "        line = pd.DataFrame({'x': [start_node['x'], end_node['x']], 'y': [start_node['y'], end_node['y']]})\n",
    "        fig.add_trace(px.line_mapbox(line, lat=\"y\", lon=\"x\").data[0])\n",
    "\n",
    "    # Customize the map layout\n",
    "    fig.update_layout(mapbox_style=\"light\",\n",
    "                    mapbox_accesstoken=\"pk.eyJ1IjoiZ3VwdGFzYW16IiwiYSI6ImNsZ3d6Zzh0eTAwbjMzcW8wcnJybmp6cmcifQ.4ZGZIjNSFzk6aYjYUT3P1Q\",  # Replace with your Mapbox access token\n",
    "                    mapbox_center={\"lat\": 36.1627, \"lon\": -86.7816},\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin={\"b\": 20, \"l\": 5, \"r\": 5, \"t\": 40},\n",
    "                    annotations=[\n",
    "                        dict(text=\"Python code: <a href='https://plotly.com/ipython-notebooks/network-graphs/'>https://plotly.com/ipython-notebooks/network-graphs/</a>\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002)\n",
    "                    ],\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "\n",
    "    op_fp = './plots/dynamic_graph_1_hr_time_window/graph_strucutre/with_mapbox'\n",
    "    pathlib.Path(op_fp).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "\n",
    "    # Save or display the figure\n",
    "    fig.write_html(f'{op_fp}/graph_with_mapbox_{transit_date}_{time_window}.html')\n",
    "\n",
    "def plot_and_save_graph_without_mapbox(G,pos_df,transit_date,time_window):\n",
    "    # pos = nx.spiral_layout(G)\n",
    "    pos = pos_df.to_dict(orient='index')\n",
    "    pos = {key: tuple(value.values()) for key, value in pos.items()}\n",
    "\n",
    "    fig = plt.figure(1, figsize=(200, 80), dpi=60)\n",
    "    nx.draw_networkx(G,font_size=100,node_size=30000, arrowsize=300, width=10,pos=pos)\n",
    "\n",
    "    op_fp = './plots/dynamic_graph_1_hr_time_window/graph_strucutre/without_mapbox'\n",
    "    pathlib.Path(op_fp).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "    plt.savefig(f'{op_fp}/graph_{transit_date}_{time_window}.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "t = apc_data[(apc_data.transit_date == '2021-11-05')]\n",
    "\n",
    "graphs = []\n",
    "count = 0\n",
    "time_window_count = 0\n",
    "# Creating the road network graph for a single days data for now. Here each node represents the stop and edge represents the stop that the bus travels to.\n",
    "# for (route_direction_name), tdf in tqdm(df.groupby(['route_direction_name' ])):   \n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(t.groupby(['transit_date', 'route_direction_name' , 'time_window_1_hr'])):   \n",
    "    # display(tdf)\n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # print('RDN: ',route_direction_name)\n",
    "    G=nx.from_pandas_edgelist(tdf, 'source', 'target', ['displacement','median_income_last12months',\n",
    "                                                    'white_pct', 'black_pct', 'hispanic_pct', 'public_transit_pct',\n",
    "                                                    'pct_public_transit_for_work'],create_using=nx.DiGraph())\n",
    "    \n",
    "    try:\n",
    "        G.remove_node('DELETE_NODE')\n",
    "    except:\n",
    "        print('iteration:',count)\n",
    "\n",
    "    graphs.append(G)\n",
    "    \n",
    "    pos_df = tdf.groupby(['stop_id']).agg({\n",
    "                                            'map_longitude':'first','map_latitude':'first'\n",
    "                                            })\n",
    "    \n",
    "    plot_and_save_graph_with_mapbox(G,pos_df,transit_date,time_window)\n",
    "\n",
    "    plot_and_save_graph_without_mapbox(G,pos_df,transit_date,time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26373/26373 [00:36<00:00, 730.24it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "graphs = []\n",
    "count = 0\n",
    "time_window_count = 0\n",
    "# Creating the road network graph for a single days data for now. Here each node represents the stop and edge represents the stop that the bus travels to.\n",
    "# for (route_direction_name), tdf in tqdm(df.groupby(['route_direction_name' ])):   \n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(apc_data.groupby(['transit_date', 'route_direction_name' , 'time_window_1_hr'])):   \n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # print('RDN: ',route_direction_name)\n",
    "    G=nx.from_pandas_edgelist(tdf, 'source', 'target', ['displacement'],create_using=nx.DiGraph())\n",
    "    \n",
    "    try:\n",
    "        G.remove_node('DELETE_NODE')\n",
    "    except:\n",
    "        print('iteration:',count)\n",
    "\n",
    "    graphs.append(G)\n",
    "    \n",
    "    pos_df = tdf.groupby(['stop_id']).agg({\n",
    "                                            'map_longitude':'first','map_latitude':'first'\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the feature node matrix for the first graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_load(load, capacity):\n",
    "    percentages = [0., .33, .66, 1.0]\n",
    "    # percentages = [0., .10, .25, 1.0]\n",
    "\n",
    "    transit_cap = [round(p * capacity) for p in percentages]\n",
    "    labels = [0, 1, 2]\n",
    "    \n",
    "    bin_label = pd.cut(x=[load], bins=transit_cap, labels=labels, include_lowest=True)[0]\n",
    "    \n",
    "    if pd.isnull(bin_label):\n",
    "        return 3\n",
    "\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26373/26373 [35:05<00:00, 12.53it/s]  \n"
     ]
    }
   ],
   "source": [
    "features = ['precipitation_intensity','temperature','humidity','delay','average_speed','extreme_congestion',\n",
    "            'dayofweek','month','year','time_window_1_hr'] \n",
    "node_feature_matrix_graphs = []\n",
    "y_class_graphs = []\n",
    "# time_window_index = []\n",
    "\n",
    "iteration = 0\n",
    "for (transit_date, route_direction_name, time_window), tdf in tqdm(apc_data.groupby(['transit_date','route_direction_name' ,'time_window_1_hr'])):   \n",
    "    # Create node feature matrix and the y_class matrix (Creating this as well as a dictionary for now)\n",
    "    # Designed as a dictionary. Traverse each row sequentially and get the aggregate (mean) of all the values \n",
    "    node_feature_matrix = {}\n",
    "    y_class = {}\n",
    "\n",
    "    if route_direction_name == 1:\n",
    "        continue\n",
    "\n",
    "    # Setting the keys to maintain the sequential order with respect to nodes in graph\n",
    "    for key in list(graphs[iteration].nodes):\n",
    "        node_feature_matrix[key] = None\n",
    "        y_class[key] = None\n",
    "\n",
    "    for (stop_id), stop_df in tdf.groupby(['stop_id']):\n",
    "        stop_id = stop_df['stop_id'].iloc[0]\n",
    "\n",
    "        node_feature_matrix[stop_id] = {}\n",
    "        # Getting the route_direction_name taking only the first value because this will be the same for all the stops in the graph (The graph have been grouped on this column)\n",
    "\n",
    "        # Aggregate (mean) the Weather data - temp, precipitation, humidity. \n",
    "        node_feature_matrix[stop_id]['precipitation_intensity'] = stop_df.precipitation_intensity.mean()\n",
    "        node_feature_matrix[stop_id]['temperature'] = stop_df.temperature.mean()\n",
    "        node_feature_matrix[stop_id]['humidity'] = stop_df.humidity.mean()\n",
    "\n",
    "        # node_feature_matrix[stop_id]['actual_hdwy'] = stop_df.actual_hdwy.mean()\n",
    "        node_feature_matrix[stop_id]['delay'] = stop_df.delay.mean()\n",
    "\n",
    "        # Aggregate Traffic data after merging.\n",
    "        node_feature_matrix[stop_id]['average_speed'] = stop_df.average_speed.mean()\n",
    "        node_feature_matrix[stop_id]['extreme_congestion'] = stop_df.extreme_congestion.mean()\n",
    "\n",
    "        # Time features\n",
    "        node_feature_matrix[stop_id]['dayofweek'] = stop_df.dayofweek_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['month'] = stop_df.month_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['year'] = stop_df.year_cat.iloc[0]\n",
    "        node_feature_matrix[stop_id]['time_window_1_hr'] = stop_df.time_window_1_hr_cat.iloc[0] #Check whether this need to be one hot encoded \n",
    "\n",
    "        # Getting the binned load using mean load and mean vehicle capacity\n",
    "        mean_load = stop_df.load.max()\n",
    "        mean_vehicle_capacity = stop_df.vehicle_capacity.max()\n",
    "        binned_load = bin_load(mean_load,mean_vehicle_capacity)\n",
    "\n",
    "        # Getting the mean of the load - will bin it later using vehicle capacity\n",
    "        y_class[stop_id] = binned_load\n",
    "\n",
    "    node_feature_matrix_graphs.append(node_feature_matrix)\n",
    "    y_class_graphs.append(y_class)\n",
    "    iteration = iteration+1\n",
    "\n",
    "    # time_window_index.append(time_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import pickle\n",
    "    with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/node_feature_matrix_13_features.pkl\", 'wb') as fp:\n",
    "        pickle.dump(node_feature_matrix_graphs, fp)\n",
    "\n",
    "    with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/y_class_13_features.pkl\", 'wb') as fp:\n",
    "        pickle.dump(y_class_graphs, fp)\n",
    "\n",
    "    with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/graph_13_features.pkl\", 'wb') as fp:\n",
    "        pickle.dump(graphs, fp)\n",
    "except:\n",
    "    print(\"Could not save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "\n",
    "# with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/node_feature_matrix_10_features.pkl\", 'rb') as fp:\n",
    "#     node_feature_matrix_graphs = pickle.load(fp)\n",
    "\n",
    "\n",
    "# with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/y_class_10_features.pkl\", 'rb') as fp:\n",
    "#     y_class_graphs = pickle.load(fp)\n",
    "\n",
    "\n",
    "# with open(\"./data/graph_data/1_hr_time_window_dynamic_graph/graph_10_features.pkl\", 'rb') as fp:\n",
    "#     graphs = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_class to list from dictionary\n",
    "y_class = list(y_class.values())\n",
    "\n",
    "# Converting node_feature_matrix to 2D matrix from dictionary of dictionaries\n",
    "for key in node_feature_matrix:\n",
    "    node_feature_matrix[key] = list(node_feature_matrix[key].values())\n",
    "node_feature_matrix = list(node_feature_matrix.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix shape: (0,)\n",
      "Number of nodes: 54\n",
      "Number of Features:  10\n"
     ]
    }
   ],
   "source": [
    "# Below value should equal (number of nodes, number of features)\n",
    "print(\"Feature Matrix shape:\",np.array(node_feature_matrix).shape)\n",
    "print(\"Number of nodes:\",len(G.nodes))\n",
    "print('Number of Features: ',len(features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1347/13567 [00:00<00:00, 13457.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREVANSM\n",
      "GREMAIEM\n",
      "DICGRANN\n",
      "DICEVANN\n",
      "VAIQUAEN\n",
      "EAGDOVSN\n",
      "OAKSTASN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "VILMOUEF\n",
      "VILBRIEM\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "RICCRESM\n",
      "EAGDOVSN\n",
      "CREOAKSN\n",
      "EAGDOVSN\n",
      "FREVANSM\n",
      "VAIBRIEF\n",
      "MAIFREEF\n",
      "EAGDOVSN\n",
      "CREOAKSN\n",
      "EAGDOVSN\n",
      "EWIBRIWN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "VILBRIEM\n",
      "CREPARWN\n",
      "EAGDOVSN\n",
      "BRICHEEM\n",
      "MOOCRISN\n",
      "VAIBRIEM\n",
      "EAGDOVSN\n",
      "OAKSTASN\n",
      "EAGDOVSN\n",
      "EWIBRIWN\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "VILSTAEM\n",
      "EAGDOVSN\n",
      "MOOCRISN\n",
      "MOOCRISN\n",
      "BRICHENN\n",
      "EAGDOVSN\n",
      "CHERAISN\n",
      "MOOCRISN\n",
      "MOOWILSF\n",
      "EAGDOVSN\n",
      "OAKSTASN\n",
      "EAGDOVSN\n",
      "MOOWILSF\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "VILBRIEM\n",
      "EAGDOVSN\n",
      "RICCRESM\n",
      "EAGDOVSN\n",
      "VILSTAEM\n",
      "EAGDOVSN\n",
      "MOOCRISN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "BRICHENN\n",
      "BRIBRIEM\n",
      "LAFFAIEN\n",
      "BRIRICNN\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "DOVOAKWN\n",
      "CREOAKSN\n",
      "DOVMOOWN\n",
      "CRERICWN\n",
      "CREOAKSM\n",
      "CREPARWM\n",
      "RICCRESM\n",
      "CHEWOOSN\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "MOOCRISN\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "CREOAKSN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "CREOAKSN\n",
      "EAGDOVSN\n",
      "VILSTAEM\n",
      "BRICHEEM\n",
      "BRICHEEN\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n",
      "MURFRASN\n",
      "MXODONEL\n",
      "MXOPLUSP\n",
      "MURDELEN\n",
      "MURTOWEF\n",
      "EAGDOVSN\n",
      "VAISPEEM\n",
      "EAGDOVSN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2693/13567 [00:00<00:00, 13323.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAM8AWN\n",
      "VILSTAEM\n",
      "LAFFAIEN\n",
      "HER12AWN\n",
      "BAT29AWN\n",
      "HER14AWN\n",
      "JAM8AWN\n",
      "HAR8AWF\n",
      "HERDRISF\n",
      "DICCLENN\n",
      "DICLUCNF\n",
      "DICMARNM\n",
      "CHA7AWN\n",
      "LAFFAIEN\n",
      "MXOELMHI\n",
      "JAM8AWN\n",
      "HER12AWN\n",
      "BAT29AWN\n",
      "HER14AWN\n",
      "DOVOAKWN\n",
      "HER12AWN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 4026/13567 [00:00<00:00, 12450.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWIBRIWN\n",
      "RICVAIWN\n",
      "BRIBRIEM\n",
      "MOOCRISN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "EWIBRIWN\n",
      "VILBRIEM\n",
      "EAGDOVSN\n",
      "MOOWILSF\n",
      "CREPARWM\n",
      "EAGDOVSN\n",
      "VAIBRIEF\n",
      "EAGDOVSN\n",
      "BAT29AWN\n",
      "VILSTAEM\n",
      "EAGDOVSN\n",
      "BRICHEEN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "RICVAIWN\n",
      "EAGDOVSN\n",
      "RICVAIWN\n",
      "EAGDOVSN\n",
      "6AVDEASN\n",
      "6AVCHUSN\n",
      "WALLINNN\n",
      "HARLINEF\n",
      "LINWALWM\n",
      "DICBRONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 5331/13567 [00:00<00:00, 12667.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARTAMEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 7801/13567 [00:00<00:00, 11716.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXOTHOMP\n",
      "MXOPLUSP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 8979/13567 [00:00<00:00, 10920.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CXONGULC\n",
      "CXO17AVE\n",
      "MORBRISN\n",
      "EAGDOVSN\n",
      "WALTUREN\n",
      "5AVHARNN\n",
      "DICDOVSN\n",
      "33AJOHSN\n",
      "EAGDOVSN\n",
      "DOVMOOWN\n",
      "DICDOVSN\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "FREVANSM\n",
      "EWIEWIWN\n",
      "EAGDOVSN\n",
      "OAKSTASN\n",
      "EAGDOVSN\n",
      "VILBRIEM\n",
      "GREATHEM\n",
      "BRIMASNN\n",
      "BRICHENN\n",
      "EAGDOVSN\n",
      "EAGDOVSN\n",
      "33AJOHSN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 12033/13567 [00:01<00:00, 9210.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RICEWISM\n",
      "WES27AWN\n",
      "4AVCOMSN\n",
      "BRIRICNN\n",
      "8AMADNM\n",
      "1SWOONM\n",
      "5SRUSSN\n",
      "LINWALWM\n",
      "6AVCHUSN\n",
      "WESACKWN\n",
      "WES31AWN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13567/13567 [00:01<00:00, 10442.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2AGAYNM\n",
      "3AVMADNF\n",
      "WHAHAYEN\n",
      "HPKL\n",
      "6AVDEASN\n",
      "WHAHAYEN\n",
      "HPKL\n",
      "WHAHAYEN\n",
      "HPKL\n",
      "DOVDICWF\n",
      "WHAHAYEN\n",
      "HPKL\n",
      "21ABROSF\n",
      "4AVCOMSN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in tqdm(range(len(node_feature_matrix_graphs))):\n",
    "    # Converting y_class to list from dictionary\n",
    "    y_class_graphs[iteration] = list(y_class_graphs[iteration].values())\n",
    "\n",
    "    # Converting node_feature_matrix to 2D matrix from dictionary of dictionaries\n",
    "    for key in node_feature_matrix_graphs[iteration]:\n",
    "        try:\n",
    "            node_feature_matrix_graphs[iteration][key] = list(node_feature_matrix_graphs[iteration][key].values())\n",
    "        except:\n",
    "            print(key)\n",
    "    node_feature_matrix_graphs[iteration] = list(node_feature_matrix_graphs[iteration].values())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs:  13567\n",
      "Number of node_feature_matrix_graphs:  13567\n",
      "Number of y_class_graphs:  13567\n"
     ]
    }
   ],
   "source": [
    "# Print some stats\n",
    "print(\"Number of graphs: \",len(graphs))\n",
    "print(\"Number of node_feature_matrix_graphs: \",len(node_feature_matrix_graphs))\n",
    "print(\"Number of y_class_graphs: \",len(y_class_graphs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the networkx graph to pygeometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing values for None rows \n",
    "\n",
    "for i in range(len(node_feature_matrix_graphs)):\n",
    "    for j in range(len(node_feature_matrix_graphs[i])):\n",
    "        # print(node_feature_matrix_graphs[i][j])\n",
    "        if(node_feature_matrix_graphs[i][j] is None):\n",
    "            node_feature_matrix_graphs[i][j] = np.zeros(len(features))\n",
    "        else:\n",
    "            node_feature_matrix_graphs[i][j] = np.array(node_feature_matrix_graphs[i][j])\n",
    "\n",
    "\n",
    "for i in range(len(y_class_graphs)):\n",
    "    # print(y_class)\n",
    "    for j in range(len(y_class_graphs[i])):\n",
    "        if(y_class_graphs[i][j] is None):\n",
    "            y_class_graphs[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1249227, '1': 698934, '2': 98306, '3': 27857, '4': 0, 'None': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict = {\n",
    "    '0':0,\n",
    "    '1':0,\n",
    "    '2':0,\n",
    "    '3':0,\n",
    "    '4':0,\n",
    "    'None':0\n",
    "}\n",
    "\n",
    "for i in range(len(y_class_graphs)):\n",
    "    # print(y_class)\n",
    "    for j in range(len(y_class_graphs[i])):\n",
    "        y_dict[str(y_class_graphs[i][j])] += 1\n",
    "\n",
    "y_dict\n",
    "\n",
    "# {'0': 1014097, '1': 588765, '2': 83207, '3': 25561, '4': 0, 'None': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13567/13567 [00:14<00:00, 918.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These graphs were removed because there was only a single node in these graphs: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the resulting dataframe with all the pygeometric graph data\n",
    "dataset_all_routes = []\n",
    "\n",
    "count = 0\n",
    "for iteration in tqdm(range(len(graphs))):\n",
    "\n",
    "    try:\n",
    "        # Creating the py geometric graph from networkx graph\n",
    "        pyg_graph = from_networkx(graphs[iteration],group_edge_attrs=['displacement'])\n",
    "        # Setting the node feature matrix for the py geometric graph \n",
    "        pyg_graph.x = torch.tensor(node_feature_matrix_graphs[iteration]).float()\n",
    "        # Setting the y_class for the py geometric graph \n",
    "        pyg_graph.y = torch.tensor(y_class_graphs[iteration]).long()\n",
    "\n",
    "        dataset_all_routes.append(pyg_graph)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        count = count + 1\n",
    "        continue\n",
    "        # print(\"Some issue with graph: \",iteration)\n",
    "\n",
    "print(\"These graphs were removed because there was only a single node in these graphs:\",count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Number of graphs: 13525\n",
      "Number of features: 10\n",
      "Number of classes: 4\n",
      "\n",
      "Data(edge_index=[2, 25], edge_attr=[25, 1], num_nodes=26, x=[26, 10], y=[26])\n",
      "=============================================================\n",
      "Number of nodes: 26\n",
      "Number of edges: 25\n",
      "Average node degree: 0.96\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = dataset_all_routes[0]  # Get the first graph object.\n",
    "# Hard coding for now the code after ends here can be used to derieve this\n",
    "num_classes = 4\n",
    "num_features = len(features)\n",
    "\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset_all_routes)}')\n",
    "print(f'Number of features: {num_features}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! Creating Data\n"
     ]
    }
   ],
   "source": [
    "# Saving the final data\n",
    "op_path = './data/graph_data/1_hr_time_window_dynamic_graph'\n",
    "op_filename = 'dataset_all_route_dynamic_graph_10_features.pt'\n",
    "\n",
    "torch.save(dataset_all_routes, f'{op_path}/{op_filename}')\n",
    "\n",
    "print(\"DONE! Creating Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
